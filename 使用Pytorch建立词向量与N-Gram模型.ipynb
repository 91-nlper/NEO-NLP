{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这个数据预处理过程没有做，应该将句子根据标点符号分开，然后处理标点符号；\n",
    "test_sentence = \"\"\"\n",
    "A gifted American psychologist has said ,“Worry is a spasm of the emotion; \n",
    "the mind catches hold of something and will not let it go.” It is useless \n",
    "to argue with the mind in this condition. The stronger the will, the more \n",
    "futile the task. One can only gently insinuate something else into its \n",
    "convulsive grasp. And if this something else is rightly chosen, if it is \n",
    "really attended by the illumination of another field of interest, gradually, \n",
    "and often swiftly, the old undue grip relaxes and the process of recuperation \n",
    "and repair ,begins.The cultivation of a hobby and new forms of interest \n",
    "is therefore a policy of first importance to a public man. But this is not \n",
    "a business that can be undertaken in a day or swiftly improvised by a mere \n",
    "command of the will. The growth of alternative mental interests is a long \n",
    "process. The seeds must be carefully chosen; they must fall on good ground; \n",
    "they must be sedulously tended, if the vivifying fruits are to be at hand \n",
    "when needed. To be really happy and really safe, one ought to have at least \n",
    "two or three hobbies, and they must all be real. It is no use starting late \n",
    "in life to say: \"I will take an interest in this or that.\" Such an attempt \n",
    "only aggravates the strain of mental effort. A man may acquire great knowledge \n",
    "of topics unconnected with his daily work, and yet hardly get any benefit or \n",
    "relief. It is no use doing what you like: you have got to like what you do. \n",
    "Broadly speaking, human beings may be divided into three classes: those who \n",
    "are toiled to death, those who are worried to death, and those who are bored \n",
    "to death. It is no use offering the manual labourer, tired out with a hard \n",
    "week's sweat and effort，the chance of playing a game of football or baseball \n",
    "on Saturday afternoon. It is no use inviting the politician or the professional \n",
    "or businessman, who has been working or worrying about serious things for six \n",
    "days, to work or worry about trifling things at the week-end.\"\"\".split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据标准化及字典构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把材料构造成三元语法语料\n",
    "trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) \n",
    "            for i in range(len(test_sentence) - 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造字典（单词及其对应的所有）\n",
    "vocabulary = set(test_sentence)\n",
    "word_to_ix = {word: i for i, word in enumerate(vocabulary)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TriGram(nn.Module):\n",
    "    def __init__(self, vocabulary_size, embedding_dim, context_size, n_hidden):\n",
    "        super(TriGram, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dim)\n",
    "        # print(\"the original embedding is : \", self.embedding.weight)  初始化的词向量矩阵\n",
    "        self.Linear1 = nn.Linear(context_size * embedding_dim, n_hidden)\n",
    "        self.Linear2 = nn.Linear(n_hidden, vocabulary_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs).view(1, -1)\n",
    "        out = F.relu(self.Linear1(embeds))\n",
    "        out = self.Linear2(out)\n",
    "        log_probs = F.log_softmax(out, dim=1)  # 输出归一化的概率\n",
    "        return log_probs\n",
    "\n",
    "losses = []\n",
    "loss_function = nn.NLLLoss()\n",
    "model = TriGram(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, 100)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    total_loss = 0\n",
    "    for context, target in trigrams:\n",
    "        context_idx = [word_to_ix[word] for word in context]  # 产生单词对应的索引号\n",
    "        context_var = torch.LongTensor(context_idx)           # 在model.embedding中查询时索\n",
    "                                                              #引必须是tensor数据类型的列表\n",
    "        log_probs = model(context_var)\n",
    "        loss = loss_function(log_probs, torch.LongTensor([word_to_ix[target]]))\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data.numpy()  # 将torch张量转换成python数据类型\n",
    "    losses.append(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练后的参数查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35.974687576293945,\n",
       " 35.900949478149414,\n",
       " 35.82237243652344,\n",
       " 35.75556659698486,\n",
       " 35.696616649627686,\n",
       " 35.644667625427246,\n",
       " 35.57563257217407,\n",
       " 35.572914123535156,\n",
       " 35.48055648803711,\n",
       " 35.43225288391113]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 损失函数的值记录\n",
    "losses[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.3247,  0.9301, -1.3573,  ..., -0.3592, -0.2334,  1.2196],\n",
      "        [ 0.9144, -0.9955,  1.5296,  ..., -0.2890,  0.8897,  0.2187],\n",
      "        [-1.6678,  0.2426, -0.1024,  ..., -0.3030,  0.2784, -0.2243],\n",
      "        ...,\n",
      "        [ 1.2325, -1.1330,  0.5589,  ...,  0.8175,  1.0662,  1.6292],\n",
      "        [ 1.6744,  0.3188, -0.0541,  ...,  1.3325, -0.1892,  1.6094],\n",
      "        [ 0.9994,  1.4078, -2.1743,  ...,  0.8051,  0.7900,  0.9466]],\n",
      "       requires_grad=True)\n",
      "torch.Size([212, 20])\n",
      "--------------------------------\n",
      "Parameter containing:\n",
      "tensor([[ 0.1344, -0.1569, -0.0925,  ...,  0.1955, -0.1501, -0.5005],\n",
      "        [-0.2007, -0.1836,  0.1702,  ...,  0.1511, -0.2214,  0.2219],\n",
      "        [-0.2359, -0.1666, -0.0539,  ...,  0.0777, -0.4448,  0.0176],\n",
      "        ...,\n",
      "        [ 0.1075,  0.5467, -0.4375,  ..., -0.7483,  0.0772,  0.0534],\n",
      "        [ 0.4177,  0.2394,  0.3525,  ..., -0.2348,  0.1393, -0.1571],\n",
      "        [ 0.0417, -0.1167, -0.1147,  ..., -0.4520,  0.3573, -0.2919]],\n",
      "       requires_grad=True)\n",
      "torch.Size([100, 40])\n",
      "--------------------------------\n",
      "Parameter containing:\n",
      "tensor([ 0.3417,  0.0799,  0.0787,  0.0465, -0.0839,  0.1796,  0.1107,  0.0796,\n",
      "         0.1746,  0.0470,  0.2095,  0.1097,  0.1270,  0.1059,  0.2431,  0.1312,\n",
      "         0.3521,  0.2048, -0.1498,  0.0427,  0.2579, -0.0553,  0.2642,  0.1838,\n",
      "         0.2098,  0.5502, -0.1736,  0.1842,  0.1716,  0.1263,  0.0128,  0.1539,\n",
      "         0.0523,  0.2444,  0.1494,  0.3339,  0.0141, -0.0013,  0.0607, -0.0536,\n",
      "         0.4779, -0.0023,  0.1450, -0.1523,  0.1245, -0.0264,  0.0372,  0.3636,\n",
      "         0.0445,  0.2555,  0.0539,  0.1794, -0.1377, -0.0102,  0.2609,  0.5183,\n",
      "         0.2583,  0.2272,  0.2248,  0.2783,  0.0476, -0.0204, -0.0086,  0.1712,\n",
      "        -0.0311,  0.1500,  0.3177,  0.0927, -0.0735,  0.5013,  0.1949,  0.3384,\n",
      "         0.3713,  0.1934,  0.0505,  0.2514,  0.0757, -0.2514, -0.0008, -0.2948,\n",
      "        -0.0023,  0.1097,  0.1735,  0.2038, -0.0554,  0.0081,  0.2574,  0.1302,\n",
      "         0.0687,  0.2637,  0.2215,  0.1553,  0.0984,  0.1006,  0.0662,  0.2443,\n",
      "         0.3543, -0.0782,  0.3117, -0.0682], requires_grad=True)\n",
      "torch.Size([100])\n",
      "--------------------------------\n",
      "Parameter containing:\n",
      "tensor([[ 0.0410, -0.1478, -0.0230,  ...,  0.2018,  0.1765,  0.0013],\n",
      "        [ 0.0494,  0.0752,  0.1864,  ..., -0.0876, -0.1208, -0.0935],\n",
      "        [-0.0413,  0.0218, -0.0370,  ..., -0.1196,  0.0682, -0.0194],\n",
      "        ...,\n",
      "        [ 0.1933, -0.1495, -0.0491,  ...,  0.1422, -0.0593,  0.1911],\n",
      "        [-0.2634, -0.1716,  0.0555,  ...,  0.2420,  0.0243,  0.2002],\n",
      "        [ 0.1325, -0.1838,  0.2421,  ..., -0.2133, -0.1335,  0.1486]],\n",
      "       requires_grad=True)\n",
      "torch.Size([212, 100])\n",
      "--------------------------------\n",
      "Parameter containing:\n",
      "tensor([-0.0091, -0.0429, -0.0256, -0.0403, -0.0774, -0.0430,  0.1241, -0.0378,\n",
      "        -0.1864,  0.0138,  0.0036,  0.0712, -0.0554,  0.1525, -0.0782,  0.0302,\n",
      "         0.0098, -0.0446, -0.1765, -0.0099, -0.0979,  0.1630,  0.0276, -0.1161,\n",
      "         0.0150, -0.1400, -0.0327,  0.1798, -0.1228,  0.0118,  0.0988, -0.0565,\n",
      "        -0.0465, -0.0074,  0.1921, -0.0758, -0.0422, -0.0525, -0.0748, -0.0906,\n",
      "        -0.0992,  0.0627, -0.0390, -0.0854,  0.0277, -0.0377, -0.0875, -0.0476,\n",
      "         0.1127, -0.0048, -0.1511, -0.0989,  0.0395,  0.0817, -0.0472,  0.0740,\n",
      "         0.0248, -0.1229, -0.0946,  0.1196, -0.1229, -0.1008, -0.0159,  0.0148,\n",
      "        -0.0007,  0.0617,  0.0106, -0.0507,  0.0251, -0.0730,  0.1104, -0.0547,\n",
      "        -0.1021,  0.0046,  0.1813, -0.0427, -0.1654, -0.0374, -0.0078, -0.0548,\n",
      "        -0.0278,  0.1106,  0.2066,  0.0152,  0.0399,  0.0309, -0.0624,  0.1011,\n",
      "        -0.0626, -0.0973,  0.2007,  0.0052,  0.0141, -0.0467, -0.0566, -0.0015,\n",
      "         0.0264, -0.0455,  0.2419, -0.0724, -0.0399,  0.1290, -0.0413, -0.0757,\n",
      "         0.1206,  0.0286,  0.0727,  0.2257,  0.0535, -0.0239,  0.0192,  0.2026,\n",
      "        -0.0568, -0.1443, -0.0514,  0.0315, -0.0549, -0.0431,  0.0335,  0.0810,\n",
      "         0.0026, -0.1694,  0.0149,  0.0585, -0.0176, -0.1107, -0.0061, -0.1176,\n",
      "        -0.1164, -0.0431,  0.1129,  0.1421,  0.0438, -0.0923, -0.0795, -0.0557,\n",
      "         0.0572, -0.0480, -0.0390, -0.0603,  0.1156,  0.0175,  0.1362,  0.1619,\n",
      "         0.0299,  0.4173, -0.0422, -0.0350,  0.0280,  0.1635,  0.0027,  0.0490,\n",
      "         0.0062, -0.1140,  0.1121, -0.1391, -0.0253, -0.0996,  0.0100,  0.1372,\n",
      "        -0.1042,  0.0239, -0.0739,  0.0043,  0.0356,  0.2143,  0.0044, -0.1103,\n",
      "        -0.0295, -0.0006, -0.1070, -0.0304, -0.0151, -0.0780,  0.0887,  0.1149,\n",
      "         0.0525, -0.0200,  0.0962,  0.1081, -0.0679,  0.1429, -0.1129, -0.1579,\n",
      "        -0.0369,  0.0966, -0.0358, -0.0593, -0.0894,  0.1401, -0.2281,  0.4481,\n",
      "         0.0318,  0.0306,  0.0299,  0.0036, -0.1256, -0.0024,  0.0186, -0.0918,\n",
      "         0.0389,  0.0043, -0.0442, -0.0758,  0.0058, -0.0162,  0.2658, -0.0133,\n",
      "        -0.0385, -0.0595, -0.1099,  0.2078], requires_grad=True)\n",
      "torch.Size([212])\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 所有已训练的参数查看\n",
    "for i in model.parameters():\n",
    "    print(i)\n",
    "    print(i.size())\n",
    "    print(\"--------------------------------\")\n",
    "# 第一部分是词向量矩阵参数\n",
    "# 第二部分是第一个隐藏层神经网络参数\n",
    "# 第三部分是第一个隐藏层神经元参数\n",
    "# 第四部分是第二个隐藏层神经网络参数\n",
    "# 第五部分是第二个隐藏层神经元参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3260e+01, -1.6079e+01, -1.4686e+01, -1.3130e+01, -1.3127e+01,\n",
       "         -1.3303e+01, -1.2622e+01, -1.4933e+01, -1.2443e+01, -1.5048e+01,\n",
       "         -2.3489e-03, -1.5741e+01, -1.4513e+01, -8.5724e+00, -1.2366e+01,\n",
       "         -1.5997e+01, -1.6008e+01, -1.6568e+01, -1.7033e+01, -1.3998e+01,\n",
       "         -1.1984e+01, -8.3298e+00, -1.5376e+01, -1.7780e+01, -1.7302e+01,\n",
       "         -1.5183e+01, -1.5807e+01, -1.3663e+01, -1.4328e+01, -1.4960e+01,\n",
       "         -1.3636e+01, -1.3667e+01, -1.5580e+01, -1.6290e+01, -1.4054e+01,\n",
       "         -1.6862e+01, -1.5225e+01, -1.3880e+01, -1.3614e+01, -1.3389e+01,\n",
       "         -1.8145e+01, -1.0662e+01, -1.1804e+01, -1.6987e+01, -1.2963e+01,\n",
       "         -1.6250e+01, -1.6539e+01, -1.2751e+01, -1.4974e+01, -1.5406e+01,\n",
       "         -1.7230e+01, -1.1295e+01, -1.2057e+01, -1.5164e+01, -1.2064e+01,\n",
       "         -1.3241e+01, -1.5192e+01, -1.6498e+01, -1.7651e+01, -8.5439e+00,\n",
       "         -1.3256e+01, -1.5602e+01, -1.5434e+01, -1.5380e+01, -1.8464e+01,\n",
       "         -1.7216e+01, -1.6582e+01, -1.2419e+01, -1.2036e+01, -1.6573e+01,\n",
       "         -1.5115e+01, -1.1142e+01, -1.5273e+01, -1.3046e+01, -8.3153e+00,\n",
       "         -1.6905e+01, -1.2837e+01, -1.0518e+01, -1.7085e+01, -1.4988e+01,\n",
       "         -1.4953e+01, -1.6076e+01, -1.2831e+01, -1.6698e+01, -1.4717e+01,\n",
       "         -9.1915e+00, -1.3074e+01, -1.2748e+01, -1.5193e+01, -1.6867e+01,\n",
       "         -1.3892e+01, -1.4427e+01, -1.7232e+01, -1.6584e+01, -1.5843e+01,\n",
       "         -1.5248e+01, -1.3703e+01, -1.4145e+01, -9.0169e+00, -1.1900e+01,\n",
       "         -1.8126e+01, -1.0068e+01, -1.1417e+01, -1.6901e+01, -1.3036e+01,\n",
       "         -1.5700e+01, -1.3385e+01, -1.4894e+01, -1.2986e+01, -1.2311e+01,\n",
       "         -1.7233e+01, -1.4934e+01, -1.1139e+01, -1.3939e+01, -1.5779e+01,\n",
       "         -9.5531e+00, -1.6547e+01, -1.3775e+01, -1.6341e+01, -1.7572e+01,\n",
       "         -1.5038e+01, -1.9071e+01, -1.5423e+01, -1.6508e+01, -1.4564e+01,\n",
       "         -9.0678e+00, -1.5030e+01, -1.3422e+01, -1.8060e+01, -1.3974e+01,\n",
       "         -1.4564e+01, -1.4330e+01, -1.3250e+01, -1.7745e+01, -1.5948e+01,\n",
       "         -1.1187e+01, -1.4671e+01, -1.0807e+01, -1.3286e+01, -1.1347e+01,\n",
       "         -1.4338e+01, -1.7048e+01, -1.0030e+01, -1.1743e+01, -1.5824e+01,\n",
       "         -1.1109e+01, -1.2368e+01, -1.4477e+01, -1.3710e+01, -1.7202e+01,\n",
       "         -1.6274e+01, -1.4224e+01, -1.7220e+01, -1.8213e+01, -9.0150e+00,\n",
       "         -1.7399e+01, -9.3111e+00, -1.5064e+01, -1.6564e+01, -1.2752e+01,\n",
       "         -1.6810e+01, -1.3421e+01, -1.4139e+01, -1.8308e+01, -1.4241e+01,\n",
       "         -1.3840e+01, -1.5932e+01, -1.6810e+01, -1.3554e+01, -1.7471e+01,\n",
       "         -1.2909e+01, -1.1969e+01, -1.4823e+01, -1.5761e+01, -9.8111e+00,\n",
       "         -1.7442e+01, -1.0498e+01, -1.6216e+01, -1.4700e+01, -9.2008e+00,\n",
       "         -1.3337e+01, -1.3556e+01, -1.6244e+01, -1.7301e+01, -1.4591e+01,\n",
       "         -1.4311e+01, -1.3075e+01, -1.4695e+01, -1.7849e+01, -1.5392e+01,\n",
       "         -1.1929e+01, -1.1422e+01, -1.5622e+01, -1.0021e+01, -1.3702e+01,\n",
       "         -1.5492e+01, -1.5878e+01, -1.4785e+01, -1.2745e+01, -1.3996e+01,\n",
       "         -1.2689e+01, -1.4625e+01, -1.4035e+01, -1.1359e+01, -1.7962e+01,\n",
       "         -1.0514e+01, -1.7760e+01, -1.3743e+01, -1.6004e+01, -1.3732e+01,\n",
       "         -1.2398e+01, -8.8930e+00]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"worry\", \"about\"]\n",
    "idxes = [word_to_ix[word] for word in words]\n",
    "var = torch.LongTensor([idxes])\n",
    "log_pro = model(var)\n",
    "log_pro  # 是一个二维矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = log_pro.data.numpy().tolist()[0]  # [0] 选择维度\n",
    "index = m.index(max(m))  # 最大的参数的索引即为所预测的单词\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trifling\n"
     ]
    }
   ],
   "source": [
    "for word, identify in word_to_ix.items():\n",
    "    if identify == index:\n",
    "        print(word)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
